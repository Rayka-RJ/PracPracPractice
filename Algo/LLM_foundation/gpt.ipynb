{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfca4aa",
   "metadata": {},
   "source": [
    "# basic GPT\n",
    "\n",
    "这个 notebook 将按照 Karpathy 的教程，从零开始实现一个简化版的 GPT 语言模型。\n",
    "\n",
    "### Topic\n",
    "\n",
    "- **总体目标**：  \n",
    "  - 手搓一个最小可用的 GPT（解码器-only Transformer），能在简单文本语料上进行自回归生成。\n",
    "\n",
    "- **数据与预处理**：  \n",
    "  - 选择一份小型文本数据集（如 Shakespeare、Linux 源码片段、中文语料等）。  \n",
    "  - 文本清洗、拆分训练集 / 验证集。  \n",
    "  - 构建 tokenizer（可以从最简单的字符级 / BPE / WordPiece 开始）。  \n",
    "  - 将文本转为 token 序列，并按照固定 `block_size` 切成训练样本。\n",
    "\n",
    "- **模型结构（简化 GPT）**：  \n",
    "  - Token embedding 与 position embedding 的实现与相加。  \n",
    "  - **多头自注意力（Multi-Head Self-Attention）**：  \n",
    "    - 计算 Query / Key / Value，注意力分数与 softmax。  \n",
    "    - 使用 causal mask（上三角 mask）保证自回归（只能看到过去的 token）。  \n",
    "    - 多头拆分与拼接，线性投影输出。  \n",
    "  - 前馈网络（Feed-Forward Network, MLP）层：  \n",
    "    - 两层线性 + 非线性激活（如 GELU）。  \n",
    "  - 残差连接与 LayerNorm：  \n",
    "    - 每个子层（注意力、MLP）前后加上 LayerNorm 和残差。  \n",
    "  - 多层 Transformer block 堆叠，构成完整 GPT 解码器。  \n",
    "  - 输出层线性映射到词表维度，得到每个 token 的 logits。\n",
    "\n",
    "- **训练流程**：  \n",
    "  - 构建 mini-batch 数据加载（随机采样连续序列）。  \n",
    "  - 定义交叉熵损失（Cross-Entropy Loss）。  \n",
    "  - 使用 AdamW 等优化器。  \n",
    "  - 学习率调度与梯度裁剪（如需要）。  \n",
    "  - 简单的训练日志与 loss 可视化。\n",
    "\n",
    "- **推理与采样**：  \n",
    "  - 给定 prompt，循环自回归生成下一个 token。  \n",
    "  - 支持 temperature、top-k / top-p 采样策略。  \n",
    "  - 观察模型生成质量，尝试不同参数的效果。\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- **数据需求**：  \n",
    "  - 一份中小规模纯文本数据集（几 MB ～ 数十 MB 即可）。  \n",
    "  - 文本编码统一为 UTF-8。  \n",
    "  - 能够放入内存或通过数据加载器分批读取。\n",
    "\n",
    "- **计算资源**：  \n",
    "  - 推荐：支持 CUDA 的 GPU（显存 ≥ 8GB 更舒适），方便更快实验。  \n",
    "  - 也可在 CPU 上运行，但训练时间会显著变长。  \n",
    "  - 足够磁盘空间保存数据与模型权重（几十到几百 MB 级别）。\n",
    "\n",
    "- **软件环境**：  \n",
    "  - 核心依赖：  \n",
    "    - `torch`（或 `torch.cuda` 环境，如果有 GPU）  \n",
    "    - `numpy`  \n",
    "    - `tqdm`（进度条显示，非必需但推荐）  \n",
    "    - `matplotlib` 或 `seaborn`（若需要简单可视化）  \n",
    "  - 可选依赖：  \n",
    "    - `datasets`（Hugging Face 数据集工具，用于快速加载公开语料）。  \n",
    "    - `tokenizers`（若想使用更高效的 BPE/WordPiece tokenizer）。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb99511",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Data preprocessing\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
